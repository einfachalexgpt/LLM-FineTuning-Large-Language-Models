# LLM (Large Language Models) FineTuning Projects and notes on common practical techniques

![](/assets/devil2.png)

### Find me here..

- 🐦 TWITTER: https://twitter.com/rohanpaul_ai
- 🟠 YouTube: https://www.youtube.com/@RohanPaul-AI/featured
- 👨🏻‍💼 LINKEDIN: https://www.linkedin.com/in/rohan-paul-b27285129/
- ​👨‍🔧​ KAGGLE: https://www.kaggle.com/paulrohan2020

---

[logo]: https://raw.githubusercontent.com/rohan-paul/MachineLearning-DeepLearning-Code-for-my-Youtube-Channel/master/assets/yt_logo.png


### Fine-tuning LLM (and YouTube Video Explanations)

| Notebook | 🟠 **YouTube Video**|
| -------- | ---------------------- |
| [Finetune Llama-3-8B with unsloth 4bit quantized with ORPO](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/Llama_3_Finetuning_ORPO_with_Unsloth.ipynb) | [![Youtube Link][logo]](https://www.youtube.com/watch?v=6ikUpJcDrPs&list=PLxqBkZuBynVTzqUQCQFgetR97y1X_1uCI&index=31) |
| [Llama-3 Finetuning on custom dataset with unsloth](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/Llama-3_Finetuning_on_custom_dataset_with_unsloth.ipynb) | [![Youtube Link][logo]](https://www.youtube.com/watch?v=AmVEGPS9JIg&list=PLxqBkZuBynVTzqUQCQFgetR97y1X_1uCI&index=25) |
| [CodeLLaMA-34B - Conversational Agent ](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/CodeLLaMA_34B_Conversation_with_Streamlit.py) | [![Youtube Link][logo]](https://www.youtube.com/watch?v=815NpXvniIg&list=PLxqBkZuBynVTzqUQCQFgetR97y1X_1uCI&index=16&ab_channel=Rohan-Paul-AI) |
| [Inference Yarn-Llama-2-13b-128k with KV Cache to answer quiz on very long textbook](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/Inference_Yarn-Llama-2-13b-128k_Github.ipynb) | [![Youtube Link][logo]](https://www.youtube.com/watch?v=RYTOQERqVsg&list=PLxqBkZuBynVTzqUQCQFgetR97y1X_1uCI&index=14&ab_channel=Rohan-Paul-AI)|
| [Mistral 7B FineTuning with_PEFT and QLORA](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/Mistral_FineTuning_with_PEFT_and_QLORA.ipynb) | [![Youtube Link][logo]](https://www.youtube.com/watch?v=6DGYj1EEWOw&list=PLxqBkZuBynVTzqUQCQFgetR97y1X_1uCI&index=13&ab_channel=Rohan-Paul-AI)|
| [Falcon finetuning on openassistant-guanaco](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/Falcon-7B_FineTuning_with_PEFT_and_QLORA.ipynb) | [![Youtube Link][logo]](https://www.youtube.com/watch?v=fEzuBFi35J4&list=PLxqBkZuBynVTzqUQCQFgetR97y1X_1uCI&index=11&ab_channel=Rohan-Paul-AI)|
| [Fine Tuning Phi 1_5 with PEFT and QLoRA](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/FineTuning_phi-1_5_with_PRFT_LoRA.ipynb) | [![Youtube Link][logo]](https://www.youtube.com/watch?v=J0RbOtLrJhQ&list=PLxqBkZuBynVTzqUQCQFgetR97y1X_1uCI&index=10&ab_channel=Rohan-Paul-AI)|
| [Web scraping with Large Language Models (LLM)-AnthropicAI + LangChainAI](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/Web%20scraping%20with%20Large%20Language%20Models%20(LLM)-AnthropicAI%20%2B%20LangChainAI.ipynb) | [![Youtube Link][logo]](https://www.youtube.com/watch?v=QAY82UvrsHg&list=PLxqBkZuBynVTiTEvP6-GYf35yA6OqIN7Y&index=2&ab_channel=Rohan-Paul-AI)|


---------------------------

### Fine-tuning LLM

| Notebook | Colab |
| -------- | ------------- |
| 📌 [Gemma_2b_finetuning_ORPO_full_precision](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/gemma-2b_ORPO_FineTuning_full_precision/v2_Colab_Gemma_2b_orpo.ipynb)|<a href="https://colab.research.google.com/github/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/gemma-2b_ORPO_FineTuning_full_precision/Gemma_2b_orpo_full_precision_Colab.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
| 📌 [Jamba_Finetuning_Colab-Pro](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/tinyllama_fine-tuning_Taylor_Swift.ipynb)|<a href="https://colab.research.google.com/github/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/Jamba_Finetuning_Colab-Pro.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
| 📌 [Finetune codellama-34B with QLoRA](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/Finetune_codellama-34B-with-QLoRA.ipynb)|<a href="https://colab.research.google.com/github/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/Finetune_codellama-34B-with-QLoRA.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
| 📌 [Mixtral Chatbot with Gradio](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/Mixtral_Chatbot_with_Gradio)|
| 📌 [togetherai api to run Mixtral](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/togetherai-api-with_Mixtral.ipynb)|<a href="https://colab.research.google.com/github/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/togetherai-api-with_Mixtral.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
| 📌 [Integrating TogetherAI with LangChain 🦙](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/TogetherAI_API_with_LangChain.ipynb)|<a href="https://colab.research.google.com/github/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/TogetherAI_API_with_LangChain.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
| 📌 [Mistral-7B-Instruct_GPTQ - Finetune on finance-alpaca dataset 🦙](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/Mistral-7B-Instruct_GPTQ-finetune.ipynb)|<a href="https://colab.research.google.com/github/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/Mistral_7B_Instruct_GPTQ_finetune.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
| 📌 [Mistral 7b FineTuning with DPO Direct_Preference_Optimization](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/Mistral_7b_FineTuning_with_DPO_Direct_Preference_Optimization.ipynb)|<a href="https://colab.research.google.com/github/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/Mistral_7b_FineTuning_with_DPO_Direct_Preference_Optimization.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
| 📌 [Finetune llama_2_GPTQ](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/Finetune_llama_2_GPTQ)
| 📌 [TinyLlama with Unsloth and_RoPE_Scaling dolly-15 dataset](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/TinyLlama_with_Unsloth_and_RoPE_Scaling_dolly-15k.ipynb)|<a href="https://colab.research.google.com/github/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/TinyLlama_with_Unsloth_and_RoPE_Scaling_dolly-15k.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
| 📌 [Tinyllama fine-tuning with Taylor_Swift Song lyrics](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/tinyllama_fine-tuning_Taylor_Swift.ipynb)|<a href="https://colab.research.google.com/github/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/tinyllama_fine-tuning_Taylor_Swift.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>




---------------------------

### LLM Techniques and utils - Explained

| LLM Concepts |
| -------- |
| 📌 [DPO (Direct Preference Optimization) training and its datasets](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/LLM_Techniques_and_utils/DPOTrainer.ipynb)|
| 📌 [4-bit LLM Quantization with GPTQ](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/LLM_Techniques_and_utils/4-bit_LLM_Quantization_with_GPTQ.ipynb)|
| 📌 [Quantize with HF Transformers](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/Quantize_with_HF_transformers)|
| 📌 [Understanding rank r in LoRA and related Matrix_Math](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/LLM_Techniques_and_utils/Understanding_rank_r_in_LoRA_and_related_Matrix_Math.ipynb)|
| 📌 [Rotary Embeddings (RopE) is one of the Fundamental Building Blocks of LlaMA-2 Implementation](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/LLM_Techniques_and_utils/RoPE-As-Implemented-in-LlaMa-Source-Code.ipynb)|
| 📌 [Chat Templates in HuggingFace](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/LLM_Techniques_and_utils/apply_chat_template.ipynb)|
| 📌 [How is Mixtral 8x7B is a dense 47Bn param model](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/LLM_Techniques_and_utils/MOE-Mixture-of-Experts/Mixtral_8x7B_MoE_Why_47Bn_param_by_Shared_Param.md)|
| 📌 [The concept of **validation log perplexity** in LLM training - a note on fundamentals.](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/LLM_Techniques_and_utils/Validation_log_perplexity.md)|
| 📌 [Why we need to identify `target_layers` for LoRA/QLoRA](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/LLM_Techniques_and_utils/targets_layers_in_peft_and_meaning_of_Rank_of_a_Matrix.ipynb)|
| 📌 [Evaluate Token per sec](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/LLM_Techniques_and_utils/Evaluate_token_per_sec.ipynb)|
| 📌 [traversing through nested attributes (or sub-modules) of a PyTorch module](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/LLM_Techniques_and_utils/Traverse_through_sub-modules_of_PyTorch_Model.ipynb)|
| 📌 [Implementation of Sparse Mixtures-of-Experts layer in PyTorch from Mistral Official Repo](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/LLM_Techniques_and_utils/MoE_implementation_Mistral_official_Repo.ipynb)|
| 📌 [Util method to extract a specific token's representation from the last hidden states of a transformer model.](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/LLM_Techniques_and_utils/Select_last_meaningful_token_from_each_sequence.ipynb)|
| 📌 [Convert PyTorch model's parameters and tensors to half-precision floating-point format](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/LLM_Techniques_and_utils/Convert_Pytorch_model_to_half_precision.ipynb)|
| 📌 [Quantizing 🤗 Transformers models with the GPTQ method](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/LLM_Techniques_and_utils/Quantizing_Transformers_with_GPTQ.ipynb)|
| 📌 [Quantize Mixtral-8x7B so it can run in 24GB GPU](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/LLM_Techniques_and_utils/Quantize_mixtral-instruct-awq_in_so_it_can_run_in_24GB.ipynb)|
| 📌 [What is GGML or GGUF in the world of Large Language Models ?](https://github.com/rohan-paul/LLM-FineTuning-Large-Language-Models/blob/main/LLM_Techniques_and_utils/GGUF_GGML_GPTQ-basics.md)|





